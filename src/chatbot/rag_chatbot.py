import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_groq import ChatGroq

# Charge le fichier .env automatiquement
load_dotenv()

# Configuration
DB_DIR = os.path.join("data", "chroma_db")

# --- CL√â API GROQ (lue depuis .env ou variable d'environnement) ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "gsk_...")

def load_db():
    if not os.path.exists(DB_DIR):
        print(f"Erreur : La base de donn√©es {DB_DIR} n'existe pas. Lancez d'abord ingest_knowledge.py")
        return None

    # On utilise toujours le m√™me embedding pour la recherche
    embedding_function = SentenceTransformerEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    db = Chroma(persist_directory=DB_DIR, embedding_function=embedding_function)
    return db

def get_llm_response(db, query, api_key):
    if not api_key or api_key == "gsk_...":
        return "Erreur : Cl√© API Groq manquante. V√©rifiez votre fichier .env ou d√©finissez la variable GROQ_API_KEY."

    # 1. Recherche des documents pertinents
    docs = db.similarity_search(query, k=3)
    
    if not docs:
        return "D√©sol√©, je n'ai pas trouv√© d'information dans ma base de connaissances."
    
    # 2. Construction du contexte
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # 3. Configuration du LLM (Llama 3.3 via Groq)
    llm = ChatGroq(
        temperature=0, 
        groq_api_key=api_key, 
        model_name="llama-3.3-70b-versatile"
    )

    # 4. Construction du prompt (bilingue Fran√ßais / Darija)
    prompt = f"""Tu es un assistant virtuel expert pour Orange Maroc. Tu peux parler en fran√ßais et en darija (arabe marocain).

R√àGLE IMPORTANTE : D√©tecte la langue de la question de l'utilisateur et r√©ponds DANS LA M√äME LANGUE.
- Si la question est en fran√ßais ‚Üí r√©ponds en fran√ßais, de mani√®re polie et professionnelle.
- Si la question est en darija (arabe marocain, parfois m√©lang√© avec du fran√ßais) ‚Üí r√©ponds en darija, de mani√®re sympa et naturelle.

Si tu ne connais pas la r√©ponse, dis-le honn√™tement sans inventer.

Utilise le contexte suivant pour r√©pondre :
{context}

Question : {query}

R√©ponse :"""

    # 5. Appel au LLM
    try:
        response = llm.invoke(prompt)
        return response.content
    except Exception as e:
        return f"Erreur avec Groq : {str(e)}"

# Fonction simplifi√©e pour Streamlit (r√©tro-compatibilit√©)
def get_response(db, query):
    global GROQ_API_KEY
    return get_llm_response(db, query, GROQ_API_KEY)

def chat():
    global GROQ_API_KEY
    db = load_db()
    if db is None:
        return

    print("\n--- CHATBOT ORANGE (IA Llama 3.3) ---")
    print("Posez votre question (ou tapez 'quit' pour quitter)\n")
    
    while True:
        query = input("\nVous : ")
        if query.lower() in ['quit', 'exit', 'q']:
            break
        
        print("ü§ñ R√©flexion en cours...", end="\r")
        response = get_llm_response(db, query, GROQ_API_KEY)
        print(f"ü§ñ Bot : {response}\n")

if __name__ == "__main__":
    chat()
